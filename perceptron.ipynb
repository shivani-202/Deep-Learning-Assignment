{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfaFqrbsovX1HcYXtT6tTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivani-202/Deep-Learning-Assignment/blob/main/perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Design a single unit perceptron for classification of a linearly separable binary dataset without using pre-defined models.\n",
        "\n",
        "2) Use the Perceptron from sklearn.\n",
        "Identify the problem with single unit Perceptron.\n",
        "\n",
        "* Classify using OR-, And- and XOR-ed data and analyze the result.\n",
        "\n",
        "* Multiclass  classification task: Classify MNIST dataset  and analyze the result"
      ],
      "metadata": {
        "id": "waUAL4_n7edb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCPsDmef3E0Q",
        "outputId": "40337fe8-c854-4ec6-e93b-705c18ed4a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on AND dataset:\n",
            "Predictions: [0 0 0 1]\n",
            "Actual Labels: [0 0 0 1]\n",
            "\n",
            "Training on OR dataset:\n",
            "Predictions: [0 1 1 1]\n",
            "Actual Labels: [0 1 1 1]\n",
            "\n",
            "Training on XOR dataset:\n",
            "Predictions: [1 1 0 0]\n",
            "Actual Labels: [0 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, max_epochs=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def step_function(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "            for i in range(len(X)):\n",
        "                # Compute linear combination\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
        "\n",
        "                # Applying step function\n",
        "                y_pred = self.step_function(linear_output)\n",
        "\n",
        "                # Calculating error\n",
        "                error = y[i] - y_pred\n",
        "\n",
        "                # Updating weights and bias\n",
        "                self.weights += self.learning_rate * error * X[i]\n",
        "                self.bias += self.learning_rate * error\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.array([self.step_function(x) for x in linear_output])\n",
        "\n",
        "# Defining datasets\n",
        "AND_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "AND_labels = np.array([0, 0, 0, 1])\n",
        "\n",
        "OR_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "OR_labels = np.array([0, 1, 1, 1])\n",
        "\n",
        "XOR_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "XOR_labels = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Training perceptron on AND dataset\n",
        "perceptron = Perceptron(learning_rate=0.1, max_epochs=10)\n",
        "print(\"Training on AND dataset:\")\n",
        "perceptron.fit(AND_data, AND_labels)\n",
        "AND_predictions = perceptron.predict(AND_data)\n",
        "print(\"Predictions:\", AND_predictions)\n",
        "print(\"Actual Labels:\", AND_labels)\n",
        "\n",
        "# Training perceptron on OR dataset\n",
        "print(\"\\nTraining on OR dataset:\")\n",
        "perceptron.fit(OR_data, OR_labels)\n",
        "OR_predictions = perceptron.predict(OR_data)\n",
        "print(\"Predictions:\", OR_predictions)\n",
        "print(\"Actual Labels:\", OR_labels)\n",
        "\n",
        "# Training perceptron on XOR dataset\n",
        "print(\"\\nTraining on XOR dataset:\")\n",
        "perceptron.fit(XOR_data, XOR_labels)\n",
        "XOR_predictions = perceptron.predict(XOR_data)\n",
        "print(\"Predictions:\", XOR_predictions)\n",
        "print(\"Actual Labels:\", XOR_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It correctly classify AND and OR datasets (linearly separable).\n",
        "It fails on the XOR dataset (non-linearly separable)."
      ],
      "metadata": {
        "id": "ls0YDz5QJCaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "print(\"MNIST dataset...\")\n",
        "data = pd.read_csv('/content/sample_data/mnist_train_small.csv')\n",
        "\n",
        "data_test = pd.read_csv('/content/sample_data/mnist_test.csv')\n",
        "\n",
        "data.head(), data_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYN9I3hFI9M6",
        "outputId": "42442f67-1665-406c-a8ce-0f286be94963"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   6  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.581  0.582  0.583  \\\n",
              " 0  5  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 1  7  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 2  9  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 3  5  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 4  2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " \n",
              "    0.584  0.585  0.586  0.587  0.588  0.589  0.590  \n",
              " 0      0      0      0      0      0      0      0  \n",
              " 1      0      0      0      0      0      0      0  \n",
              " 2      0      0      0      0      0      0      0  \n",
              " 3      0      0      0      0      0      0      0  \n",
              " 4      0      0      0      0      0      0      0  \n",
              " \n",
              " [5 rows x 785 columns],\n",
              "    7  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.658  0.659  0.660  \\\n",
              " 0  2  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 1  1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 2  0  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 3  4  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " 4  1  0    0    0    0    0    0    0    0    0  ...      0      0      0   \n",
              " \n",
              "    0.661  0.662  0.663  0.664  0.665  0.666  0.667  \n",
              " 0      0      0      0      0      0      0      0  \n",
              " 1      0      0      0      0      0      0      0  \n",
              " 2      0      0      0      0      0      0      0  \n",
              " 3      0      0      0      0      0      0      0  \n",
              " 4      0      0      0      0      0      0      0  \n",
              " \n",
              " [5 rows x 785 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = data.iloc[:, 0].values  # Training labels\n",
        "X_train = data.iloc[:, 1:].values  # Training features\n",
        "y_test = data_test.iloc[:, 0].values  # Test labels\n",
        "X_test = data_test.iloc[:, 1:].values  # Test features\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Perceptron model\n",
        "print(\"Training Perceptron...\")\n",
        "perceptron = Perceptron(max_iter=1000, eta0=1.0, random_state=42)\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = perceptron.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy on MNIST test set:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZuf48_M11-4",
        "outputId": "9a4fb439-481b-407c-d5ef-6cdffd235b7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perceptron...\n",
            "\n",
            "Accuracy on MNIST test set: 0.8772877287728773\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.95       980\n",
            "           1       0.92      0.98      0.95      1135\n",
            "           2       0.90      0.86      0.88      1032\n",
            "           3       0.84      0.90      0.87      1010\n",
            "           4       0.85      0.90      0.88       982\n",
            "           5       0.84      0.80      0.82       892\n",
            "           6       0.92      0.90      0.91       958\n",
            "           7       0.85      0.91      0.88      1027\n",
            "           8       0.83      0.80      0.82       974\n",
            "           9       0.87      0.74      0.80      1009\n",
            "\n",
            "    accuracy                           0.88      9999\n",
            "   macro avg       0.88      0.88      0.87      9999\n",
            "weighted avg       0.88      0.88      0.88      9999\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary of Analysis**\n",
        "Accuracy: **87.7%**, with good performance on simple, linearly separable digits like '0', '1', and '6'. It struggled with '5', '8' and '9' due to structural similarities and confusion.\n",
        "\n",
        "**Strengths**:\n",
        "   - High precision and recall for clearly defined digits ('0', '1').\n",
        "   - Weighted and macro averages of **0.88** indicates consistent performance across classes.\n",
        "\n",
        "**Weaknesses**:\n",
        "   - Struggled with non-linearly separable digits like '5', '8', and '9' due to the perceptron's inability to learn non-linear decision boundaries.\n",
        "   - Lower recall for digits with overlapping features, e.g., '9' (recall: **0.74**) and '8'.\n",
        "\n",
        "  The perceptron performed well for linearly separable data but is limited for complex datasets like MNIST.\n",
        "  We can use a non-linear model such as a multi-layer perceptron (MLP) or a convolutional neural network (CNN)."
      ],
      "metadata": {
        "id": "7Zg_1Yp94L7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "y_train = data.iloc[:, 0].values  # Training labels\n",
        "X_train = data.iloc[:, 1:].values  # Training features\n",
        "y_test = data_test.iloc[:, 0].values  # Test labels\n",
        "X_test = data_test.iloc[:, 1:].values  # Test features\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initializing MLPClassifier\n",
        "print(\"Training Multi-Layer Perceptron...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128, 64),  # Two hidden layers with 128 and 64 neurons\n",
        "                     activation='relu',           # ReLU activation function\n",
        "                     solver='adam',               # Adam optimizer\n",
        "                     max_iter=100,                # Maximum number of iterations\n",
        "                     random_state=42)\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Evaluating performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy on MNIST test set:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVZ3-NO64s6U",
        "outputId": "c0af0a90-7fcd-412b-b219-ffbfd06b831e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Multi-Layer Perceptron...\n",
            "\n",
            "Accuracy on MNIST test set: 0.9657965796579658\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.98       980\n",
            "           1       0.98      0.99      0.99      1135\n",
            "           2       0.96      0.96      0.96      1032\n",
            "           3       0.96      0.97      0.96      1010\n",
            "           4       0.97      0.96      0.96       982\n",
            "           5       0.96      0.96      0.96       892\n",
            "           6       0.97      0.97      0.97       958\n",
            "           7       0.96      0.97      0.96      1027\n",
            "           8       0.96      0.94      0.95       974\n",
            "           9       0.96      0.96      0.96      1009\n",
            "\n",
            "    accuracy                           0.97      9999\n",
            "   macro avg       0.97      0.97      0.97      9999\n",
            "weighted avg       0.97      0.97      0.97      9999\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of MLP Analysis**\n",
        "Performance: Achieved 96.6% accuracy, with consistently high precision, recall, and F1-scores (~0.97) across all classes.\n",
        "\n",
        "\n",
        "*   Multilayer Perceptron effectively handles non-linear decision boundaries, resolving limitations of the single-layer perceptron. It performs exceptionally well for clear and distinct digits like '0', '1', and '6'.\n",
        "*   Slight misclassification for visually similar digits like '8' and '9'.\n",
        "*   Higher computational complexity compared to simpler models.\n",
        "\n",
        "**Conclusion**: The MLP is a robust model for MNIST, offering significant improvements in accuracy and generalization. Further enhancements could involve deeper architectures like CNNs."
      ],
      "metadata": {
        "id": "jENStXse55rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Increasing number of layers in multilayer perceptron\n",
        "\n",
        "y_train = data.iloc[:, 0].values\n",
        "X_train = data.iloc[:, 1:].values\n",
        "y_test = data_test.iloc[:, 0].values\n",
        "X_test = data_test.iloc[:, 1:].values\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize MLPClassifier\n",
        "print(\"Training Multi-Layer Perceptron...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32),  # Added more layers: 256 -> 128 -> 64 -> 32 neurons\n",
        "                     activation='relu',                   # ReLU activation function\n",
        "                     solver='adam',                       # Adam optimizer\n",
        "                     max_iter=100,                        # Maximum number of iterations\n",
        "                     random_state=42)\n",
        "\n",
        "mlp.fit(X_train, y_train)\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy on MNIST test set:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lMhJIoY5ge9",
        "outputId": "e95fa188-30c1-41bf-beee-487c45a3e9b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Multi-Layer Perceptron...\n",
            "\n",
            "Accuracy on MNIST test set: 0.9670967096709671\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.98       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.96      0.97      0.96      1032\n",
            "           3       0.96      0.97      0.96      1010\n",
            "           4       0.97      0.97      0.97       982\n",
            "           5       0.97      0.96      0.96       892\n",
            "           6       0.97      0.97      0.97       958\n",
            "           7       0.96      0.96      0.96      1027\n",
            "           8       0.96      0.95      0.95       974\n",
            "           9       0.97      0.96      0.96      1009\n",
            "\n",
            "    accuracy                           0.97      9999\n",
            "   macro avg       0.97      0.97      0.97      9999\n",
            "weighted avg       0.97      0.97      0.97      9999\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-Layer vs 2-Layer MLP**\n",
        "\n",
        "1. **Accuracy**:\n",
        "   - **4-Layer MLP**: 96.7%\n",
        "   - **2-Layer MLP**: 96.6%\n",
        "   - The performance difference is minimal (0.1% improvement with 4 layers).\n",
        "\n",
        "2. **Precision, Recall, F1-Score**:\n",
        "   - Both models showed strong results (~0.96-0.97), with the 4-layer model slightly outperforming the 2-layer model in handling complex digits (e.g., '8' and '9').\n",
        "\n",
        "3. **Computational Cost**:\n",
        "   - The 4-layer MLP has slightly higher computational cost, with a small improvement in accuracy.\n",
        "\n",
        "4. **Conclusion**:\n",
        "   - The 4-layer MLP performs marginally better, but both architectures are highly effective for MNIST. Adding layers provides only small gains, and more advanced models (e.g., CNNs) may yield larger improvements."
      ],
      "metadata": {
        "id": "oI1M-vKa7Iib"
      }
    }
  ]
}